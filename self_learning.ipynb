{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CNN\n",
    "from LSTM import RNN\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "import copy\n",
    "def train_func(sub_train_, model, mode, optimizer, scheduler, t):\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    pseudo_aspect_train_acc = 0\n",
    "    aspect_train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=batch_size, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    model.train()\n",
    "    for i, (text, cls, gt1,lengths) in enumerate(data):\n",
    "        # print(f'size of text: {text.size()}')\n",
    "        optimizer.zero_grad()\n",
    "        if mode !='pretrain':\n",
    "            cls = target_score(cls, t)\n",
    "        text, cls, gt1 = text.to(device), cls.to(device), gt1.to(device)\n",
    "        #output = model(text)\n",
    "        output = model(text, lengths)\n",
    "        # loss = criterion(output, cls)\n",
    "        loss = kl_criterion(torch.log(F.softmax(output, dim=-1)), cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pseudo_aspect_train_acc += (output.argmax(1) == cls.argmax(1)).sum().item()\n",
    "        aspect_train_acc += (output.argmax(1) == gt1).sum().item()\n",
    "\n",
    "            \n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), aspect_train_acc / len(sub_train_), pseudo_aspect_train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_, model, mode):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    pseudo_aspect_test_acc = 0\n",
    "    aspect_test_acc = 0\n",
    "    data = DataLoader(data_, batch_size=128, collate_fn=generate_batch)\n",
    "    pred_distribution = []\n",
    "    model.eval()\n",
    "    gt=[]\n",
    "    for text, cls, gt1,lengths in data:\n",
    "        #print(text.shape,lengths)\n",
    "        text, cls, gt1 = text.to(device), cls.to(device), gt1.to(device)\n",
    "        with torch.no_grad():\n",
    "            #output = model(text)\n",
    "            output = model(text, lengths)\n",
    "            cls = target_score(cls)\n",
    "            loss = kl_criterion(torch.log(F.softmax(output, dim=-1)), cls)\n",
    "            pseudo_aspect_test_acc += (output.argmax(1) == cls.argmax(1) ).sum().item()\n",
    "            aspect_test_acc += (output.argmax(1)  == gt1).sum().item()\n",
    "            #pred_distribution.append(torch.Tensor([softmax(o) for o in output]))\n",
    "            pred_distribution.append(output)\n",
    "            gt.append(gt1)\n",
    "    pred_distribution = torch.cat(pred_distribution, dim=0)\n",
    "    pred = pred_distribution.argmax(dim=1)\n",
    "    gt = torch.cat(gt, dim=0) \n",
    "    \n",
    "    p = precision_score(gt, pred,average='macro')\n",
    "    r = recall_score(gt, pred, average='macro')\n",
    "    f1_mac = f1_score(gt, pred, average='macro')\n",
    "    p_w = precision_score(gt, pred,average='weighted')\n",
    "    r_w = recall_score(gt, pred, average='weighted')\n",
    "    f1_w = f1_score(gt, pred, average='weighted')\n",
    "    print('mac {:.5f} {:.5f} {:.5f}'.format(p, r, f1_mac))\n",
    "    print('weighted {:.5f} {:.5f} {:.5f}'.format(p_w, r_w, f1_w))\n",
    "            \n",
    "\n",
    "    return loss / len(data_), aspect_test_acc / len(data_), pseudo_aspect_test_acc / len(data_), pred_distribution\n",
    "\n",
    "\n",
    "def generate_batch(batch):\n",
    "    label = torch.cat([entry[1].unsqueeze(0) for entry in batch])\n",
    "    text = []\n",
    "    lengths = []\n",
    "    for entry in batch:\n",
    "        length = len(entry[0])\n",
    "        lengths.append(length)\n",
    "        tmp = F.pad(torch.tensor(entry[0]), (0,100-len(entry[0])), 'constant', 0).unsqueeze(0)\n",
    "        text.append(tmp)\n",
    "        for i in range(100):\n",
    "            if tmp[0][i] >= len(wv):\n",
    "                print(tmp[i])\n",
    "\n",
    "    gt1 = torch.from_numpy(np.array([entry[2] for entry in batch]))\n",
    "    \n",
    "    text = torch.cat(text)\n",
    "\n",
    "    return text, label, gt1,lengths\n",
    "\n",
    "def read_vec(file):\n",
    "    with open(file) as f:\n",
    "        embs = f.readlines()\n",
    "    wv = dict()\n",
    "    wv['unk']=np.zeros(200)\n",
    "    for line in embs[1:]:\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        vec = np.array([float(x) for x in line[1:]])\n",
    "        wv[word] = vec\n",
    "    return wv\n",
    "\n",
    "def target_score(logits, t=1.2):\n",
    "    preds = torch.nn.Softmax(dim=-1)(logits)  # batch * class\n",
    "    weight = preds**t #/ torch.sum(preds, dim=0)\n",
    "    return (weight.t() / torch.sum(weight, dim=1)).t()\n",
    "\n",
    "def softmax(x):\n",
    "    summ = sum(math.e**(xi) for xi in x if xi>0)\n",
    "    y = [math.e**(xi)/summ if xi >0 else 0 for xi in x ]\n",
    "    return y\n",
    "\n",
    "vec_file = 'wv113.txt' #\"restaurant.200d.txt\" #\n",
    "wv = read_vec(vec_file)\n",
    "word2idx = {w: i for i,w in enumerate(wv)}\n",
    "idx2word = {i: w for i,w in enumerate(wv)}\n",
    "aspect_kw = {'location': ['street', 'block', 'avenue', 'river', 'convenient'],\n",
    "             'drinks': ['drinks', 'beverage', 'wines', 'margarita', 'sake'],\n",
    "             'food': ['food', 'spicy', 'sushi', 'pizza', 'tasty'],\n",
    "             'ambience': ['romantic', 'atmosphere', 'room', 'seating', 'small'],\n",
    "             'service': ['tips', 'manager', 'wait', 'waitress', 'servers'],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643\n",
      "[[713, 0], tensor([0.1867, 0.1947, 0.2348, 0.1882, 0.1957]), 2] 642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(8633, 200, padding_idx=0)\n",
       "  (rnn): LSTM(200, 100, num_layers=4, dropout=0.2, bidirectional=True)\n",
       "  (fc): Linear(in_features=200, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.005\n",
    "batch_size = 16\n",
    "thres = 0.0\n",
    "output_size = len(aspect_kw)\n",
    "embedding_length = 200\n",
    "N_EPOCHS_PRE = 50\n",
    "N_EPOCHS = 200\n",
    "self_training = True\n",
    "label_file = 'pseudo4_10'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "with open(os.path.join(label_file)) as f:\n",
    "    test_cont = f.readlines()\n",
    "    asp_gt, asp_labels, docs, scores = list(), list(), list() ,list()\n",
    "    for line in test_cont:\n",
    "        asp_gti, pseudo, confi0, confi1, confi2, confi3, confi4, doc = line.split('\\t')\n",
    "        asp_gt.append(int(asp_gti))\n",
    "        asp_labels.append(int(pseudo))\n",
    "        scores.append([float(confi0),float(confi1),float(confi2),float(confi3),float(confi4)])\n",
    "        docs.append(doc)\n",
    "\n",
    "total_dataset_aspect = []\n",
    "train_dataset_aspect = []\n",
    "high_conf = []\n",
    "for i, t in enumerate(docs):\n",
    "    s_index = [word2idx[w] if w in wv else 0 for w in t.split(' ')]\n",
    "    total_dataset_aspect.append([s_index, torch.tensor(scores[i]),asp_gt[i]])\n",
    "    if max(torch.tensor(scores[i]))>thres:\n",
    "        high_conf.append(i)\n",
    "        train_dataset_aspect.append([s_index, torch.tensor(scores[i]),asp_gt[i]])\n",
    "print(len(total_dataset_aspect))\n",
    "print(train_dataset_aspect[0],len(train_dataset_aspect))\n",
    "\n",
    "\n",
    "aspect_embedding = torch.zeros((len(wv), embedding_length))\n",
    "for i in range(len(wv)):\n",
    "    aspect_embedding[i] = torch.tensor(wv[idx2word[i]])\n",
    "    \n",
    "#pretrain_model = CNN(batch_size, output_size, 1, 20, [2,3,4,5], 1, 0, 0.0, len(wv), embedding_length, aspect_embedding)\n",
    "pretrain_model = RNN(len(wv), embedding_length, embedding_length//2, output_size, 4, True,0.2, aspect_embedding)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "kl_criterion = torch.nn.KLDivLoss()\n",
    "pretrain_optimizer = torch.optim.Adam(pretrain_model.parameters(), lr=learning_rate)\n",
    "pretrain_scheduler = torch.optim.lr_scheduler.StepLR(pretrain_optimizer, 1, gamma=0.9)\n",
    "pretrain_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mac 0.66044 0.62292 0.63524\n",
      "weighted 0.84791 0.84603 0.84422\n",
      "Aspect Model 0.0000002\n",
      "Total validation 0.846, total 0.953\n",
      "mac 0.67497 0.62238 0.64227\n",
      "weighted 0.84618 0.84603 0.84369\n",
      "Aspect Model 0.0000002\n",
      "Total validation 0.846, total 0.963\n",
      "1 2.488335925349927\n",
      "mac 0.72558 0.60260 0.64007\n",
      "weighted 0.84631 0.84137 0.83429\n",
      "Aspect Model 0.0000002\n",
      "Total validation 0.841, total 0.936\n",
      "2 5.44323483670296\n",
      "mac 0.71391 0.60451 0.64371\n",
      "weighted 0.84525 0.84292 0.83755\n",
      "Aspect Model 0.0000002\n",
      "Total validation 0.843, total 0.949\n",
      "3 3.732503888024885\n",
      "mac 0.70662 0.63478 0.66473\n",
      "weighted 0.84784 0.85070 0.84731\n",
      "Aspect Model 0.0000001\n",
      "Total validation 0.851, total 0.958\n",
      "4 3.8880248833592534\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-314-35464067e6fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect_train_acc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpseudo_aspect_train_acc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_aspect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aspect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#valid_loss, aspect_test_acc, pseudo_aspect_test_acc, _ = test(train_dataset_aspect, pretrain_model, 'aspect')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect_test_acc_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpseudo_aspect_test_acc_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_dataset_aspect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aspect'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-307-92502cb33023>\u001b[0m in \u001b[0;36mtrain_func\u001b[0;34m(sub_train_, model, mode, optimizer, scheduler, t)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mpseudo_aspect_train_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "update_confi = None\n",
    "total_dist = None\n",
    "for epoch in range(40):\n",
    "    start_time = time.time()\n",
    "    train_loss, aspect_train_acc,  pseudo_aspect_train_acc = \\\n",
    "    train_func(train_dataset_aspect, pretrain_model, 'aspect', pretrain_optimizer, pretrain_scheduler, t=1)\n",
    "    #valid_loss, aspect_test_acc, pseudo_aspect_test_acc, _ = test(train_dataset_aspect, pretrain_model, 'aspect')\n",
    "    _, aspect_test_acc_total, pseudo_aspect_test_acc_total, total_dist = test(total_dataset_aspect, pretrain_model, 'aspect')\n",
    "    print('Aspect Model {:.7f}'.format(train_loss))# aspect_test_acc, pseudo_aspect_test_acc)\n",
    "    print('Total validation {:.3f}, total {:.3f}'.format(aspect_test_acc_total, pseudo_aspect_test_acc_total))\n",
    "    #total_dist = target_score(total_dist)\n",
    "    update_confi, choice = torch.max(total_dist,axis=1)\n",
    "    if epoch >0:\n",
    "        label_change = (1 - torch.sum(last_choice == choice).item() / len(choice))*100\n",
    "        print(epoch, label_change)\n",
    "        #if aspect_test_acc_total>0.82:\n",
    "        #    break\n",
    "    last_choice = choice\n",
    "    \n",
    "\n",
    "# pretrain thres 0.0 lr 0.004\n",
    "# Aspect Model 0.0012521617059418903 0.6635514018691588 0.7227414330218068\n",
    "# Total validation 0.6640746500777605 0.7216174183514774\n",
    "# all\n",
    "# 0.7573872472783826\n",
    "\n",
    "\n",
    "# pretrain thres 0.0 lr 0.005 t =1\n",
    "# 0.7527216174183 \n",
    "# 0.8118 t=1.4\n",
    "\n",
    "# 110.txt\n",
    "# pretrain lr 0.005 t=50 pretrain_model = RNN(len(wv), embedding_length, embedding_length, output_size, 4, True,0.2, aspect_embedding)\n",
    "# 0.8709175738724728\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_choice = choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642\n",
      "mac 0.70233 0.62243 0.65221\n",
      "weighted 0.84992 0.85381 0.84955\n",
      "Total validation 0.85381, label acc 0.94090\n",
      "2.799377916018664\n",
      "642\n",
      "mac 0.74087 0.62972 0.66172\n",
      "weighted 0.84265 0.84292 0.83795\n",
      "Total validation 0.84292, label acc 0.94557\n",
      "6.687402799377917\n",
      "642\n",
      "mac 0.71214 0.63495 0.65665\n",
      "weighted 0.84602 0.84603 0.84201\n",
      "Total validation 0.84603, label acc 0.93468\n",
      "3.4214618973561484\n",
      "642\n",
      "mac 0.70764 0.61857 0.64399\n",
      "weighted 0.84064 0.84137 0.83644\n",
      "Total validation 0.84137, label acc 0.93779\n",
      "2.488335925349927\n",
      "642\n",
      "mac 0.75028 0.62168 0.64989\n",
      "weighted 0.85145 0.84914 0.84351\n",
      "Total validation 0.84914, label acc 0.93313\n",
      "2.799377916018664\n",
      "642\n",
      "mac 0.70975 0.61463 0.63939\n",
      "weighted 0.84497 0.84137 0.83496\n",
      "Total validation 0.84137, label acc 0.92379\n",
      "4.043545878693622\n",
      "642\n",
      "mac 0.76748 0.62619 0.66361\n",
      "weighted 0.85213 0.84914 0.84273\n",
      "Total validation 0.84914, label acc 0.92846\n",
      "1.7107309486780742\n",
      "642\n",
      "mac 0.77796 0.63917 0.67883\n",
      "weighted 0.85981 0.85848 0.85290\n",
      "Total validation 0.85848, label acc 0.93935\n",
      "2.1772939346811793\n",
      "642\n",
      "mac 0.77085 0.63329 0.67353\n",
      "weighted 0.85365 0.85537 0.85017\n",
      "Total validation 0.85537, label acc 0.93779\n",
      "2.954898911353032\n",
      "642\n",
      "mac 0.76515 0.64631 0.67652\n",
      "weighted 0.85955 0.86003 0.85569\n",
      "Total validation 0.86003, label acc 0.90980\n",
      "3.8880248833592534\n",
      "642\n",
      "mac 0.63052 0.60117 0.60441\n",
      "weighted 0.84495 0.84914 0.84379\n",
      "Total validation 0.84914, label acc 0.89580\n",
      "3.732503888024885\n",
      "642\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-316-7ac9f2355e3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0msub_dataset_aspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0masp_gt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_dataset_aspect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect_train_acc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpseudo_aspect_train_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_dataset_aspect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aspect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m#valid_loss, aspect_test_acc, pseudo_aspect_test_acc, _ = test(sub_dataset_aspect, aspect_model, 'aspect')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect_test_acc_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpseudo_aspect_test_acc_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_dataset_aspect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aspect'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-307-92502cb33023>\u001b[0m in \u001b[0;36mtrain_func\u001b[0;34m(sub_train_, model, mode, optimizer, scheduler, t)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#output = model(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# loss = criterion(output, cls)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CSE291A/proj/LSTM.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, text_lengths)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mpacked_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mpacked_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_embedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#unpack sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 573\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aspect_model = copy.deepcopy(pretrain_model)\n",
    "last_choice = pretrain_choice\n",
    "aspect_lr = 0.0005\n",
    "thres = 0.7\n",
    "thres_num = 642\n",
    "aspect_optimizer = torch.optim.Adam(aspect_model.parameters(), lr=aspect_lr)\n",
    "aspect_scheduler = torch.optim.lr_scheduler.StepLR(aspect_optimizer, 1, gamma=0.9)\n",
    "new_update_confi = update_confi\n",
    "update_index = torch.argsort(new_update_confi, descending=True)[:thres_num]\n",
    "for _ in range(50):\n",
    "    sub_dataset_aspect = []\n",
    "    '''\n",
    "    update_index =[i for i in range(len(new_update_confi)) if new_update_confi[i]>thres and new_update_confi[i]<thres+0.10]\n",
    "    print(len(update_index))\n",
    "    for i, t in enumerate(docs):\n",
    "        s_index = [word2idx[w] if w in wv else 0 for w in t.split(' ')]\n",
    "        if i in update_index:# and i not in high_conf:\n",
    "            sub_dataset_aspect.append([s_index, total_dist[i],asp_gt[i]])\n",
    "    '''\n",
    "    # reorder\n",
    "    for i in update_index:\n",
    "        t = docs[i]\n",
    "        s_index = [word2idx[w] if w in wv else 0 for w in t.split(' ')]\n",
    "        sub_dataset_aspect.append([s_index, total_dist[i],asp_gt[i]])\n",
    "    print(len(sub_dataset_aspect))    \n",
    "    train_loss, aspect_train_acc,  pseudo_aspect_train_acc = train_func(sub_dataset_aspect, aspect_model, 'aspect', aspect_optimizer, aspect_scheduler,t=1.4)\n",
    "    #valid_loss, aspect_test_acc, pseudo_aspect_test_acc, _ = test(sub_dataset_aspect, aspect_model, 'aspect')\n",
    "    _, aspect_test_acc_total, pseudo_aspect_test_acc_total, total_dist = test(total_dataset_aspect, aspect_model, 'aspect')\n",
    "    #print('Aspect Model', train_loss, aspect_test_acc, pseudo_aspect_test_acc)\n",
    "    print('Total validation {:.5f}, label acc {:.5f}'.format(aspect_test_acc_total, pseudo_aspect_test_acc_total))\n",
    "    new_update_confi, choice = torch.max(total_dist,axis=1)\n",
    "    label_change = (1 - torch.sum(last_choice == choice).item() / len(choice))*100\n",
    "    print(label_change)\n",
    "    if label_change< 1.0:\n",
    "        break\n",
    "    last_choice = choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pretrain_model,'skip113.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9044, 0.9669, 0.5842, 0.3265, 0.5674, 0.3346, 0.3634, 0.3765, 0.5489,\n",
      "        0.7045])\n",
      "tensor([471, 274, 445,  19, 351,   1, 618, 264, 416, 326])\n"
     ]
    }
   ],
   "source": [
    "top_index = torch.argsort(new_update_confi, descending=True)[:10]\n",
    "print(new_update_confi[:10])\n",
    "print(top_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0554, 0.0406, 0.1180, 0.1111, 0.6749]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =torch.tensor([[0.1818, 0.1740, 0.2007, 0.1992, 0.2443]])\n",
    "target_score(x,t=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretrain_model.load('skip113.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mac 0.70035 0.68033 0.68567\n",
      "weighted 0.84653 0.84424 0.84270\n",
      "0.8442367601246106\n"
     ]
    }
   ],
   "source": [
    "valid_loss, aspect_test_acc, pseudo_aspect_test_acc, _ = test(train_dataset_aspect, pretrain_model, 'aspect')\n",
    "print(aspect_test_acc)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_model = torch.load('skip113.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mac 0.73499 0.71394 0.72175\n",
      "weighted 0.86414 0.86449 0.86369\n",
      "0.8644859813084113\n"
     ]
    }
   ],
   "source": [
    "valid_loss, aspect_test_acc, pseudo_aspect_test_acc, _ = test(train_dataset_aspect, restore_model, 'aspect')\n",
    "print(aspect_test_acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green\n",
      "tea\n",
      "creme\n",
      "brulee\n",
      "is\n",
      "a\n",
      "must\n",
      "gt 2 label 1 pred 2\n",
      "do\n",
      "n't\n",
      "leave\n",
      "the\n",
      "restaurant\n",
      "without\n",
      "it\n",
      "gt 2 label 4 pred 2\n",
      "normally\n",
      ",\n",
      "places\n",
      "ask\n",
      "how\n",
      "hot\n",
      "you\n",
      "want\n",
      "it\n",
      ",\n",
      "but\n",
      "they\n",
      "did\n",
      "n't\n",
      "gt 4 label 2 pred 4\n",
      "great\n",
      "open\n",
      "and\n",
      "friendly\n",
      "ambience\n",
      "gt 3 label 4 pred 3\n",
      "two\n",
      "thumbs\n",
      "up\n",
      "gt 2 label 3 pred 2\n",
      "the\n",
      "cooks\n",
      "have\n",
      "been\n",
      "at\n",
      "the\n",
      "restaurant\n",
      "for\n",
      "years\n",
      "and\n",
      "cook\n",
      "family\n",
      "recipes\n",
      "gt 2 label 4 pred 2\n",
      "it\n",
      "'s\n",
      "unpretentious\n",
      "and\n",
      "gt 3 label 2 pred 3\n",
      "my\n",
      "wife\n",
      "also\n",
      "ordered\n",
      "a\n",
      "of\n",
      "hot\n",
      "water\n",
      "(\n",
      "she\n",
      "had\n",
      "a\n",
      "sore\n",
      "throat\n",
      ")\n",
      "and\n",
      "i\n",
      "guess\n",
      "that\n",
      "since\n",
      "it\n",
      "was\n",
      "only\n",
      "water\n",
      ",\n",
      "it\n",
      "was\n",
      "n't\n",
      "a\n",
      "priority\n",
      "for\n",
      "them\n",
      "to\n",
      "actually\n",
      "bring\n",
      "it\n",
      "gt 4 label 2 pred 4\n",
      "green\n",
      "tea\n",
      "creme\n",
      "brulee\n",
      "gets\n",
      "better\n",
      "each\n",
      "time\n",
      "i\n",
      "have\n",
      "it\n",
      "gt 2 label 1 pred 2\n",
      "and\n",
      "the\n",
      "service\n",
      "was\n",
      "simply\n",
      "-\n",
      "quite\n",
      "a\n",
      "delight\n",
      "gt 4 label 2 pred 4\n",
      "sit\n",
      "in\n",
      "the\n",
      "gt 3 label 0 pred 3\n",
      "i\n",
      "thought\n",
      "the\n",
      "restaurant\n",
      "was\n",
      "nice\n",
      "and\n",
      "clean\n",
      "gt 3 label 2 pred 3\n",
      "the\n",
      "dancing\n",
      ",\n",
      "white\n",
      "river\n",
      "and\n",
      "rolls\n",
      "are\n",
      "gt 2 label 0 pred 2\n",
      "for\n",
      "a\n",
      "restaurant\n",
      "with\n",
      "such\n",
      "a\n",
      "good\n",
      "reputation\n",
      "and\n",
      "that\n",
      "is\n",
      "usually\n",
      "so\n",
      "packed\n",
      ",\n",
      "there\n",
      "was\n",
      "no\n",
      "reason\n",
      "for\n",
      "such\n",
      "a\n",
      "lack\n",
      "of\n",
      "customer\n",
      "service\n",
      "gt 4 label 2 pred 4\n",
      "but\n",
      "$\n",
      "500\n",
      "for\n",
      "a\n",
      "dinner\n",
      "for\n",
      "two\n",
      "that\n",
      "did\n",
      "n't\n",
      "include\n",
      "wine\n",
      "gt 2 label 1 pred 2\n",
      "service\n",
      "was\n",
      "just\n",
      "ok\n",
      ",\n",
      "it\n",
      "is\n",
      "not\n",
      "what\n",
      "you\n",
      "'d\n",
      "expect\n",
      "for\n",
      "$\n",
      "500\n",
      "gt 4 label 2 pred 4\n",
      "she\n",
      "replied\n",
      "``\n",
      "well\n",
      "it\n",
      "would\n",
      "be\n",
      "more\n",
      "convenient\n",
      "for\n",
      "us\n",
      "if\n",
      "you\n",
      "ordered\n",
      "now\n",
      ",\n",
      "since\n",
      "you\n",
      "are\n",
      "a\n",
      "larger\n",
      "party\n",
      ",\n",
      "and\n",
      "it\n",
      "might\n",
      "get\n",
      "crowded\n",
      ".\n",
      "gt 4 label 0 pred 4\n",
      "we\n",
      "paid\n",
      "a\n",
      "fixed\n",
      "but\n",
      "got\n",
      "nothing\n",
      "!\n",
      "gt 4 label 2 pred 4\n",
      "regardless\n",
      ",\n",
      "we\n",
      "'ll\n",
      "be\n",
      "back\n",
      "and\n",
      "ca\n",
      "n't\n",
      "wait\n",
      "to\n",
      "visit\n",
      "in\n",
      "the\n",
      "summer\n",
      "to\n",
      "take\n",
      "advantage\n",
      "of\n",
      "the\n",
      "patio\n",
      "gt 3 label 4 pred 3\n",
      "going\n",
      "to\n",
      "bark\n",
      "is\n",
      "always\n",
      "worth\n",
      "the\n",
      "train\n",
      "ride\n",
      ",\n",
      "and\n",
      "will\n",
      "make\n",
      "your\n",
      "tongue\n",
      "and\n",
      "belly\n",
      "very\n",
      "happy\n",
      "gt 2 label 4 pred 2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-54542076f53c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "data = DataLoader(train_dataset_aspect, batch_size=128, collate_fn=generate_batch)\n",
    "for text, cls, gt,lengths in data:\n",
    "    output = aspect_model(text, lengths)\n",
    "    pred = output.argmax(1)\n",
    "    label = cls.argmax(1)\n",
    "    for i in range(len(gt1)):\n",
    "        if pred[i] == gt[i] and gt[i]!= label[i]:\n",
    "            for j in text[i]:\n",
    "                if j != 0:\n",
    "                    print(idx2word[int(j)])\n",
    "            print('gt {} label {} pred {}'.format(gt[i], label[i], pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_, model, mode):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    pseudo_aspect_test_acc = 0\n",
    "    aspect_test_acc = 0\n",
    "    data = DataLoader(data_, batch_size=128, collate_fn=generate_batch)\n",
    "    pred_distribution = []\n",
    "    model.eval()\n",
    "    gt=[]\n",
    "    for text, cls, gt1,lengths in data:\n",
    "        #print(text.shape,lengths)\n",
    "        text, cls, gt1 = text.to(device), cls.to(device), gt1.to(device)\n",
    "        with torch.no_grad():\n",
    "            #output = model(text)\n",
    "            output = model(text, lengths)\n",
    "            cls = target_score(cls)\n",
    "            loss = kl_criterion(torch.log(F.softmax(output, dim=-1)), cls)\n",
    "            pseudo_aspect_test_acc += (output.argmax(1) == cls.argmax(1) ).sum().item()\n",
    "            aspect_test_acc += (output.argmax(1)  == gt1).sum().item()\n",
    "            #pred_distribution.append(torch.Tensor([softmax(o) for o in output]))\n",
    "            pred_distribution.append(output)\n",
    "            gt.append(gt1)\n",
    "    pred_distribution = torch.cat(pred_distribution, dim=0)\n",
    "    pred = pred_distribution.argmax(dim=1)\n",
    "    gt = torch.cat(gt, dim=0) \n",
    "    \n",
    "    p = precision_score(gt, pred,average='macro')\n",
    "    r = recall_score(gt, pred, average='macro')\n",
    "    f1_mac = f1_score(gt, pred, average='macro')\n",
    "    p_w = precision_score(gt, pred,average='weighted')\n",
    "    r_w = recall_score(gt, pred, average='weighted')\n",
    "    f1_w = f1_score(gt, pred, average='weighted')\n",
    "    print('mac {:.5f} {:.5f} {:.5f}'.format(p, r, f1_mac))\n",
    "    print('wei"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
